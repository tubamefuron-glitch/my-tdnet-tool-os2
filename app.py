import streamlit as st
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import pandas as pd
import time
import random

st.set_page_config(page_title="TDnetæ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ” TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«")

# ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚ˆã‚Šæœ¬ç‰©ã®ãƒ–ãƒ©ã‚¦ã‚¶ã«è¿‘ã¥ã‘ã‚‹
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
}

with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    keyword = st.text_input("æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="å¢—ç”£")
    search_limit = st.slider("ãƒã‚§ãƒƒã‚¯ä»¶æ•°ï¼ˆæ–°ç€é †ï¼‰", 10, 100, 30)
    search_button = st.button("æ¤œç´¢å®Ÿè¡Œ")

@st.cache_data(ttl=600)
def get_tdnet_list():
    # æœ¬æ—¥ã®é–‹ç¤ºä¸€è¦§URL
    url = "https://www.release.tdnet.info/inbs/I_main_00.html"
    
    for attempt in range(3): # 3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹
        try:
            time.sleep(random.uniform(1, 3)) # äººé–“ã£ã½ãå°‘ã—å¾…ã¤
            res = requests.get(url, headers=HEADERS, timeout=20)
            if res.status_code != 200:
                continue
                
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, "html.parser")
            items = []
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®å–å¾—ã‚’ã‚ˆã‚Šç¢ºå®Ÿã«
            table = soup.select_one("#main-list-table")
            if not table:
                continue
                
            rows = table.find_all("tr")
            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 5: continue
                
                title_tag = cols[3].find("a")
                if title_tag and title_tag.get("href"):
                    items.append({
                        "æ™‚åˆ»": cols[0].text.strip(),
                        "ã‚³ãƒ¼ãƒ‰": cols[1].text.strip(),
                        "ç¤¾å": cols[2].text.strip(),
                        "ã‚¿ã‚¤ãƒˆãƒ«": title_tag.text.strip(),
                        "URL": "https://www.release.tdnet.info/inbs/" + title_tag.get("href")
                    })
            if items:
                return items
        except Exception as e:
            print(f"Error on attempt {attempt}: {e}")
            time.sleep(2)
            
    return []

def search_in_pdf(url, kw):
    try:
        # PDFå–å¾—æ™‚ã‚‚å°‘ã—å¾…æ©Ÿ
        time.sleep(random.uniform(0.5, 1.0))
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            with pdfplumber.open(io.BytesIO(response.content)) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if text and kw in text:
                        return i + 1
    except:
        pass
    return None

if search_button:
    all_items = get_tdnet_list()
    if not all_items:
        st.error("ç¾åœ¨ã€TDnetã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒˆå´ã§ä¸€æ™‚çš„ã«åˆ¶é™ãŒã‹ã‹ã£ã¦ã„ã‚‹ã‹ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸­ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ•°åˆ†å¾Œã«å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
    else:
        target_items = all_items[:search_limit]
        st.info(f"æœ€æ–° {len(target_items)} ä»¶ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
        
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, item in enumerate(target_items):
            progress_bar.progress((idx + 1) / len(target_items))
            status_text.text(f"èª¿æŸ»ä¸­: {item['ç¤¾å']} ({idx+1}/{len(target_items)})")
            
            page_found = search_in_pdf(item["URL"], keyword)
            if page_found:
                item["ãƒšãƒ¼ã‚¸"] = page_found
                results.append(item)
        
        status_text.empty()
        if results:
            st.success(f"ã€çš„ä¸­ã€‘ {len(results)} ä»¶ã®è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼")
            df = pd.DataFrame(results)
            st.dataframe(df, column_config={"URL": st.column_config.LinkColumn()})
        else:
            st.warning(f"ã€Œ{keyword}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¨€è‘‰ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
