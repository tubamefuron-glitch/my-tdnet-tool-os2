import streamlit as st
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import pandas as pd
import time

st.set_page_config(page_title="TDnetæ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ” TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«")

# å½è£…ãƒ–ãƒ©ã‚¦ã‚¶æƒ…å ±ï¼ˆTDnetã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å®‰å®šã•ã›ã‚‹ãŸã‚ï¼‰
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    keyword = st.text_input("æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="å¢—ç”£")
    search_limit = st.slider("ãƒã‚§ãƒƒã‚¯ä»¶æ•°ï¼ˆæ–°ç€é †ï¼‰", 10, 200, 50)
    search_button = st.button("æ¤œç´¢å®Ÿè¡Œ")

@st.cache_data(ttl=300)
def get_tdnet_list():
    url = "https://www.release.tdnet.info/inbs/I_main_00.html"
    try:
        res = requests.get(url, headers=HEADERS)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        items = []
        # TDnetã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ã‚ˆã‚ŠæŸ”è»Ÿã«å–å¾—
        table = soup.find("table", id="main-list-table")
        if not table:
            return []
        
        rows = table.find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5: continue
            
            title_tag = cols[3].find("a")
            if title_tag:
                pdf_url = "https://www.release.tdnet.info/inbs/" + title_tag.get("href")
                items.append({
                    "æ™‚åˆ»": cols[0].text.strip(),
                    "ã‚³ãƒ¼ãƒ‰": cols[1].text.strip(),
                    "ç¤¾å": cols[2].text.strip(),
                    "ã‚¿ã‚¤ãƒˆãƒ«": title_tag.text.strip(),
                    "URL": pdf_url
                })
        return items
    except Exception as e:
        st.error(f"ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def search_in_pdf(url, kw):
    try:
        # PDFå–å¾—æ™‚ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code != 200: return None
        
        with pdfplumber.open(io.BytesIO(response.content)) as pdf:
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text and kw in text:
                    return i + 1
    except:
        pass
    return None

if search_button:
    all_items = get_tdnet_list()
    if not all_items:
        st.error("TDnetã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦è©¦ã—ã¦ãã ã•ã„ã€‚")
    else:
        target_items = all_items[:search_limit]
        st.write(f"æœ€æ–° {len(target_items)} ä»¶ã®ä¸­ã‹ã‚‰ã€Œ{keyword}ã€ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã„ã¾ã™...")
        
        progress_bar = st.progress(0)
        results = []
        
        # 1ä»¶ãšã¤ã‚¹ã‚­ãƒ£ãƒ³
        placeholder = st.empty()
        for idx, item in enumerate(target_items):
            progress_bar.progress((idx + 1) / len(target_items))
            placeholder.text(f"èª¿æŸ»ä¸­({idx+1}/{len(target_items)}): {item['ç¤¾å']}")
            
            page_found = search_in_pdf(item["URL"], keyword)
            if page_found:
                item["ãƒšãƒ¼ã‚¸"] = page_found
                results.append(item)
            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã™ããªã„ã‚ˆã†ä¸€ç¬ä¼‘ã‚€
            time.sleep(0.1)
        
        placeholder.empty()
        if results:
            st.success(f"è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ {len(results)} ä»¶ãƒ’ãƒƒãƒˆ")
            df = pd.DataFrame(results)
            st.data_editor(df, column_config={"URL": st.column_config.LinkColumn()})
        else:
            st.warning(f"ã€Œ{keyword}ã€ã‚’å«ã‚€è³‡æ–™ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ä»¶æ•°ã‚’å¢—ã‚„ã—ã¦è©¦ã—ã¦ãã ã•ã„ã€‚")
